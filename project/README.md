# Сбор данных по просмотрам с видеохостингов
## Команда
Лабазкин Дмитрий & Хачатрян Екатерина

## Задача
Нужно распарсить каждую из ссылок, вытащить из неё количество просмотром и записать обратно в эксельку.

Это должен делать Airflow DAG, который выполняется каждую ночь. Каждое выполнение он проверяет, парсил ли он уже каждую ссылку за последние два дня. Это нужно, чтобы не парсить одну и ту же ссылку несколько раз.

После своего выполнения он должен писать репорт в телеграм-чат: сколько ссылок обработаны удачно, у скольких ошибка и у каких именно ошибка (номера строк в документе и сама ссылка). 

Ещё нужен мониторинг: пусть DAG пишет в Prometeus стандартные метрики для дага.

## Файлы
- dags/
  - project.py - основной код Airflow DAG
- templates/ - шаблоны для sql-запросов и определения структуры таблиц

## Решение
### Описание
- Для решения использовался функционал из нескольких библиотек, написанных в рамках курса
    - telegram_interactions (https://github.com/labdmitriy/telegram_interactions)
    - merch (https://github.com/labdmitriy/merch) 
    - scrape (https://github.com/labdmitriy/scrape)

### Шаги
1. **Обработка URLs из Google Sheets**
- Получение списка URLs из Google Sheets
- Получение списка URLs из БД PostgreSQL, которые были успешно обработаны за последние 2 дня (blacklist)
- Проверка URL на корректность по нескольким критериям
  - Формат URL корректен (если нет - ошибка с кодом incorrect_url)
  - 2 уровня домена совпадают со значениями из фиксированного списка обрабатываемых доменов (если нет - ошибка с кодом wrong_domain)
  - URL не из blacklist (если URL в blacklist - ошибка с кодом no_update)
- Сохранение результатов валидации URL в отдельный файл (url_validation_results.json)

2. **Парсинг валидных URLs**
- Для каждого из обрабатываемых доменов
  - Получить только валидные URL из файла с результатами валидации (url_validation_results.json)
  - С учетом установленной паузы между запросами, установленного для данного домена, получить и очистить значение кол-ва просмотров
    - Для некоторых доменов (Pikabu, Vimeo) для более надежного получения значения, необходимо формировать другой URL на основе оригинального
    - Сервис Vimeo, судя по всему, блокирует доступы IP-адресов с Digital Ocean и Google Cloud (https://github.com/oscarotero/Embed/issues/326), поэтому для данного сервиса для всех URLs мы получили ошибку 302. 
    Однако сама процедура парсинга также реализована и протестирована с локального компьютера.
    - Если получена сетевая ошибка - формируется ошибка с кодом {HTTP status code}_{reason}
    - Если элемент с количеством просмотров не найден - ошибка с кодом element_not_found
- Сохранение файла с результатами парсинга (url_parse_results.json)

3. **Расчет и сохранение статистики обработки URLs**
- Формирование общей статистики обработки URLs на основе файлов с результатами валидации (url_validation_results.json) и парсинга (url_parse_results.json)
- Сохранение файла с детальной информации по ошибкам
  - Номер строки (нумерация с 0)
  - URL
  - Код ошибки
- Обновление информации в БД PostgreSQL по успешно обработанным URLs
- Запись результатов в Google Sheets в соответствующую колонку команды
  - Если URL успешно был обработан в течение последних 2 дней - оставляем предыдущее значение

4. **Отправка результатов обработки URLs в Telegram**
- Формирование сообщения с вложением для отправки в Telegram
  - Сообщение - общая информация по обработке
  - Вложение - детальная информация по ошибкам
    - URLs с кодом no_update также считаются ошибками, для возможности их отслеживания в процессе обработки
  



