# Агрегация данных по покупкам из различных источников с обработкой ошибок

## Задача
В этом задании мы апгрейдим то, что получилось во второй домашке.
1. В начало пайпа добавляем оператор, который проверяет, что PostgreSQL для хранения результата доступна.
2. В случае если все ок – едем дальше по пайпу, если нет – скипаем все дальнейшие шаги.
3. Добавляем sanity-check оператор, который проверяет, что данные в порядке и их можно класть в хранилище.
4. В случае если все ок с данными – кладем в PostgreSQL, если нет – шлем уведомление в наш любимый чат в Телеге о том, что все плохо.

Требования к DAG:
- "доступность" PostgreSQL довольно растяжимое понятие – решайте сами, что вы в него вкладываете, только напишите об этом;
- при алертах в телегу нужно также передавать task_id и dag_id, но брать их из контекста, а не ручками;
- _очевидно_, что операторы, которые выполняют проверку условий в данном задании должны быть экземплярами наследников
  класса BaseBranchOperator.

## Файлы
- dags/
  - homework_4.py - основной код Airflow DAG
- https://github.com/labdmitriy/merch - библиотека с обновленным функционалом по загрузке и обработке данных 
  - Описание файлов библиотеки - в репозитории библиотеки

## Решение
### Описание
- Решение существенно переработано относительно ДЗ 2
  - Большая часть функционала организована в новой библиотеке merch
  - Для отправки сообщений в Telegram используется бот, разработанный в ДЗ 3
  - Для возможности использования универсальных способов очистки и проверки данных, а также генерации необходимых полей за одну итерацию, проверка на чистоту данных встроена в каждый из шагов. Тем самым мы можем раньше остановить процесс обработки, если данные не прошли проверку на качество.
  - Каждый этап очистки, проверки на качество и создания полей реализован с возможностью настройки гибкой конфигурации обработки для каждого из полей
### Шаги 
- Проверка доступности БД (check_db)
  - Проверка осуществляется с помощью запроса в таблицу в БД, и обрабатываются ошибки подключения (например, в случае проблемы с соединением или неверного названия БД)
- Если БД недоступна - процесс завершается (db_not_reachable)
- Для каждой сущности используется универсальный алгоритм работы с ней (process_orders, process_status, process_customers, process_goods)
  - Скачивание данных 
  - Очистка данных
    - Выбираются только колонки, которые потребуются для создания финального датасета
    - Для очистки до загрузки в БД испольуются удаление лишних пробелов и приведение определенных полей к нижнему регистру
  - Проверка очищенных данных на качество
    - Для проверки используется проверка чисел и дат на корректные форматы
    - В случае, если хотя бы одна проверка не прошла для любой строки любой сущности - сохранить информацию о DAG ID и Task ID во внешний файл, и перейти на этап отправки сообщения об ошибке в Telegram в формате:
    "Bad data. DAG: {dag_id}, Task: {task_id}", 
    после чего DAG завершается со статусом Failed
  - Создание новых полей, которые возможно создать без объединения таблиц
- Загрузка полученных данных во временные таблицы, и дальнейшие агрегация и загрузка данных в целевую таблицу (create_dataset)
- Если все этапы прошли успешно - завершаем работу DAG (all_success)
